---
title: "Class 1 - Methods 2"
author: "Pernille Brams feat. Kathrine Schulz"
date: "8/2/2024"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "Users/sari/Documents/aarhus/methods2/methods2_resources/classes")

```

### Setup
```{r}
# Make sure this guy is installed/updated (if you've alreadygot rstanarm installed, you just need to load it in using either library() or p_load() as below)
library(rstanarm)

# Load the rest
library(pacman)
pacman::p_load(tidyverse,
               ggpubr,
               ggplot2,
               stringr) # this time I'm just giving you the code
```

### Code from Chapter 1 (not exercises, just in-chapter)
```{r}
# Load data
hibbs <- read.table("C:/Users/sari/Documents/aarhus/methods2/methods2_resources/classes/data/ElectionsEconomy/data/hibbs.dat", header = TRUE)

# Make scatterplot
plot(hibbs$growth, hibbs$vote, xlab="Average recent growth in personal income",
ylab="Incumbent party's vote share")

# Estimate regression y = a + bx + error
M1 <- stan_glm(vote ~ growth, data=hibbs)

# Add a fitted line to the graph
abline(coef(M1), col="gray") # needs to be run with the plot() code above - running the whole chunk is the easiest way

# Display the fitted model
print(M1)

```

The first column shows estimates: 46.3 and 3.0 are the coefficients in the fitted line, y = 46.3 + 3.0x (see Figure 1.1b). The second column displays uncertainties in the estimates using median absolute deviations (see Section 5.3).

The last line of output shows the estimate and uncertainty of Ïƒ, the scale of the variation in the data unexplained by the regression model (the scatter of the points above and below from the regression line). This is also referred to in the book as residual standard deviation, and it is (like we know 'standard deviation' from Methods 1) a measure of the typical distance that the observed values fall from the regression line.

In Figure 1.1b, the linear model predicts vote share to roughly an accuracy of 3.9 percentage points. This means that on average, the actual values of the dependent variable (vote) deviate from the values predicted by the stan_glm model by about 3.9 percentage points.

### Code for nicer looking plot
```{r}

# Basic plot with ggplot2
ggplot(hibbs, aes(x = growth, y = vote)) +
  geom_point() +  # Add points
  labs(
    x = "Average recent growth in personal income",
    y = "Incumbent party's vote share",
    title = "Relationship between Income Growth and Vote Share",
    subtitle = "Data from Hibbs Dataset"
  ) +
  theme_minimal() +  # Use a minimal theme
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
  ) +
  geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line

```

# Exercises
These first exercises are about learning what simulating data is for.

## Ex. 0.a): In 1-2 sentences (discuss w group or reflect on your own): What does it mean to simulate data?

*generating artificial data to test statistical methods*

## Ex. 0.b): Specify some amount of data points (n), some mean and some sd, and use rnorm() to simulate this amount of data points based off of the mean and sd you've set. 
```{r}
set.seed(1998) # setting a seed (in the best year ever??) - this way, even though it's random, you'll get reproducible results next time you run this with this seed

# rnorm() works like: my_simulated_data <- rnorm(n, mean, sd) - now you go!

mydata <- rnorm(100,0,4)

```

## Ex. 0.c) Talk with your group and note in 1-2 sentences: What do we know of the data drawn from the rnorm()? 

*it's normally distributed, there are 100 values, the mean is 4, the standard deviation is 4*

## Ex. 0.d) Data are worth 10000% more if visualised, so let's visualise. Using ggplot, make a histogram visualising the data you simulated.

```{r}

#making histogram

hist_plot <- ggplot() +
  geom_histogram(aes(x=mydata), bins=10) +
  theme_minimal()

hist_plot

```


## Ex. 0.e) Calculate the empirical mean and empirical sd (just fancy words for "calculate the mean and sd of the simulated data") - are they different from the ones you've set? and why might that be?

```{r}

mean(mydata)
sd(mydata)

```

*they're a bit off, there is always incorporated error*

## Exercises from ROS
### Ex. 1.2
Sketching a regression model and data: Figure 1.1b shows data corresponding to the fitted line y = 46.3 + 3.0x with residual standard deviation 3.9, and values of x ranging roughly from 0 to 4%.
("Sketch" or "graph"= means simulate i.e. skething data means make some data yourself :) )

#### Ex. 1.2.a) Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 3.9.
* If you're not sure how to approach this exercise, try and go by the piecemeal hint-intruction in the bottom of this doc :)

```{r}
#first step
intercept <- 30
slope <- 3
error <- rnorm(100,0,sd=3.9)
x <- seq(0,4,length.out=100)

#perfect fit y
perfect_y <- intercept + slope*x

#y with error
error_y <- intercept + slope*x + error

#making dataframes
perfect_y_df <- data.frame(x = x, y = perfect_y)
error_y_df <- data.frame(x = x, y = error_y)

#plotting
ggplot(perfect_y_df, aes(x = x, y = y)) +
  geom_point() +  # Add points
  theme_minimal() +  # Use a minimal theme
  geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line

ggplot(error_y_df, aes(x = x, y = y)) +
  geom_point() +  # Add points
  theme_minimal() +  # Use a minimal theme
  geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line


```


#### Ex. 1.2.b) Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 10.

```{r}

#first step
intercept <- 30
slope <- 3
error <- rnorm(100,0,sd=10)
x <- seq(0,4,length.out=100)

#perfect fit y
perfect_y <- intercept + slope*x

#y with error
error_y <- intercept + slope*x + error

#making dataframes
perfect_y_df <- data.frame(x = x, y = perfect_y)
error_y_df <- data.frame(x = x, y = error_y)

#plotting
ggplot(perfect_y_df, aes(x = x, y = y)) +
  geom_point() +  # Add points
  theme_minimal() +  # Use a minimal theme
  geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line

ggplot(error_y_df, aes(x = x, y = y)) +
  geom_point() +  # Add points
  theme_minimal() +  # Use a minimal theme
  geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line

```


### Ex. 1.5 Goals of regression: Give examples of applied statistics problems of interest to you in which the goals are:
#### (a) Forecasting/classification -> voting results

#### (b) Exploring associations -> who vote for which party

#### (c) Extrapolation -> predicting population of a city

#### (d) Causal inference -> testing if a drug is effective 

### Ex. 2.3 Data processing: Go to the data folder Names, find the file allnames_clean.csv and use this to make a graph similar to Figure 2.8, but for girls. 

```{r}

# Get the data in (says in the book where it is)
allnames_clean <- read.csv("C:/Users/sari/Documents/aarhus/methods2/methods2_resources/classes/data/Names/data/allnames_clean.csv")

# Check out what the variables are called
colnames(allnames_clean)
unique(allnames_clean$sex)

# Get only females
female_names <- allnames_clean %>% filter(sex == 'F')

# Get the last letter in their name
female_names <- female_names %>% 
                # Making a col called last_letter
                mutate(last_letter = str_sub(female_names$name, -1))  

# Convert the new col to a factor
female_names <- female_names %>% mutate(last_letter = as.factor(last_letter))

colnames(female_names)

# Get years. The cols that are years start with X and have 4 digits after, so we can grab them using grep() and some regular expression magic
years <- grep("^X\\d{4}$", names(female_names), value = TRUE)

# Now, for each letter, let's summarise number of those per year
sums <- female_names %>% group_by(last_letter) %>% summarise_at(years, sum, na.rm = TRUE)

# Let's turn these counts (frequencies) into percentages, cause we need that for the plot. We can do this with a function
freq_to_perc <- function(x, na.rm = TRUE) (x/sum(x))*100

percentage_names_f<- mutate_at(sums, 
                               years, 
                               freq_to_perc)

# Pivot longer because it's nicer - we want one row for each year with each letter
long_df <- pivot_longer(percentage_names_f, cols = years, names_to = "Year", values_to = "number")

long_df <- long_df %>% mutate(Year = as.factor(Year))
long_df <- long_df %>% mutate(Year = as.numeric(Year))

# It goes to 1 instead of starting at 1880, so I'll add 1878 to all for a quick fix
long_df$Year <- long_df$Year+1879
max(long_df$Year) # Max should be 2010, min should be 1880

# Finally we can PLOT
long_df %>% 
  ggplot(aes(x = Year, y = number, col = last_letter, group = last_letter)) + 
  geom_line() + 
  labs(y = "Percentage of girls born with letter", 
       title = "Overview of % girls born with a specific last letter each year", 
       x = "Years") +  
  scale_x_continuous() + 
  theme(axis.text.x = element_text(angle = -45),
        plot.title = element_text(face = "bold")) # Makes title bold

```


### Ex. 2.7 (b) Reliability and validity: Give an example of a scenario of measurements that have reliability but not validity.

*a clock that is late*

# Pernille's additional notes to Chapter 1 in ROS (not mandatory to read at all)
I probably wont keep making these notes this formatted but hey, here's to get you started on methods 2 transitioning from methods 1 :) They are just notes I took when skimming through the chapter.

Some of the text below mentions a lot of new words. But e.g. learning what bayesian approaches are in detail are NOT the point of methods 2. So don't worry if it doesn't make sense yet. It's not supposed to, the focus is on Bayesian in Methods 3 and 4 - and see the TL;DR on the bottom of this doc.

## The chapter
The chapter basically explains why regression is cool: it's a method used to explore a relationship between a dependent variable and one or more independent ones. The basic idea is to find the best straight line that fits some data points. Once we have the line, and if it fits well (but not too well) we can use it to make predictions. For example, if you know someone's height (X), you can plug it into the equation Y = a+bX+e to predict their weight (Y).
- Figure 1.1 in the book shows a) incumbent party's (i.e. the political party in power at time of election) vote share in some US elections, and b) a linear regression fit to these data.

## Explanations to couple the chapter with Methods 1 and the cogsci bsc in general
Note that the book uses the 'stan_glm' function to fit models. In Methods 1 we only used lm() and lmer(), and a few of you may have used glm() in your projects for e.g. logistic regressions or multinomial. 'stan_glm' is a function from the rstanarm package in R, which is used for *Bayesian generalized linear modeling.* The package allows for fitting of what we call Bayesian models using Stan, which is a programming language designed specifically for Bayesian modelling. It uses technique like Markov Chain Monte Carlo (MCMC) to sample from probability distributions. Don't think too hard about it- you'll learn more about the details of it. For now, we just use stan_glm() to fit lines to stuff.

*1. So many new terms already!! What is the difference between "Frequentist" and "Bayesian" approaches?*
Let's take lm() and stan_glm() as our two examples to explain this. The former is frequentist and proud, the latter Bayesian and proud.

**Frequentist:**
We can use lm() for linear regression. The lm() function estimates coefficients based on the data, aiming to minimise the error between the predicted and observed values (Methods 1 stuff). We express uncertainty in the lm() through confidence intervals and p-values, which are based on the concept of repeating an experiment indefinitely. The lm() cannot incorporate any prior knowledge, and as such, the analysis we can do with this one is based solely on the current data.

**Bayesian (if you're curious to know. More will come in methods 4):**
In the Bayesian framework, we can combine prior beliefs with the data we have at hand to update our knowledge about the estimates (in lm() we can only build a model on the data we have, so if the data are flawed or just straight up wrong, then we're doomed to build a really bad model).

The function stan_glm() is the lm() counterpart, just in a Bayesian framework instead of frequentist. stan_glm() incorporates so-called "prior distributions" for the estimates and uses data to update these "beliefs", resulting in so-called "posterior distributions" (which is essentially our results; it is what the model returns to us after calculating a bunch of smart stuff for us using MCMC). In stan_glm, uncertainty is NOT expressed through p-values, but through "credible intervals" (confidence intervals better cousin) derived from the posterior distributions, providing a direct probabilistic interpretation.
Prior Information: Integrates prior knowledge or beliefs through the use of prior distributions, which are then updated with the data.

**1 What is the difference between confidence intervals and credible intervals?**
- As we have seen in Methods 1, statisticians love giving names to things :)
- One could explain the difference between the two in a very long monologue (cause they *are* fundamentally statistically different), but the key point is that a) the two terms come from two different frameworks of stats, but b) have the same overall purpose: Wider intervals (be it confidence or credible) generally indicate more uncertainty about a value. Narrow means we can be more certain about a value.

*2. Why do we all of a sudden use 'stan_glm()' and not just good ol' lm()/lmer() from Methods 1?*
The function 'stan_glm' and Bayesian models in general can be more flexible in handling complex models, such as those with non-normal error distributions or hierarchical structures, that go beyond the scope of lm() and lmer(). Furthermore, in cases with small sample sizes, rare events, or multicollinearity, Bayesian methods can give us more robust and reliable estimates. You'll use stan and bayesian in general a lot more on Methods 3 and 4, so look at the use of it here as a fancy but basically the same as lm()-work, letting you get a taste of the functions. When you run stan_glm(), you will see that it samples via MCMC, which is algorithms that sample from posterior distributions of the parameters. It offers full probabilistic inference for the model parameters, including credible intervals and posterior predictive checks, which can be more informative than point estimates and confidence intervals in traditional GLMs. If all this are new words to you, good - because we haven't taught you what all that means yet fully.

*3. What is a generalised linear model (GLM) and how is it different from the lm() / lmer()'s we're used to from Methods 1?*
A Generalised Linear Model (GLM) is "just" an extension of the traditional linear model (lm() in R for example) to accommodate response variables that are not normally distributed.

You might remember that in lm(), the response is assumed to be a linear function of the predictors with normally distributed errors (remember the lovely assumption checks?). GLMs relax these constraints in two key ways:
- Different distributions: GLMs allow for response variables to follow different distributions (like binomial for binary data (data with an outcome like TRUE/FALSE, SUCCESS/FAIL, etc.), the so-called Poisson distribution for count data (data with e.g. count of friends, count of seedlings from a field of plants, etc.).
- Link function: GLMs use a so-called link function to model the relationship between the predictors and the mean of the response distribution. You'll learn more about what this is on later semesters, but the link function basically just converts (transforms) the expected value of some response variable so that it can be modeled as a linear combination of predictors, regardless of whatever scale / constraints it came from. In linear regression (lm()), we predict the response variable directly from a linear combination of predictors. However, for many types of data (like counts, binary outcomes), this direct prediction is not appropriate because the response variable has constraints (e.g., it must be positive, or between 0 and 1). So this is what we use link functions for - and this link function can be linear (like in lm()), logistic (for binary outcomes), log (for count data), etc.

Now, the lmer() model as we know from Methods 1 extends lm() by allowing for both fixed and random effects. This is useful for hierarchical or grouped data as we've seen before (see slides from Methods 1 if you need a refresher on linear mixed effects models (lmers). Thing is that GLMs can ALSO be extended to include random effects (GLMMs), so doing GLMs is basically just a more flexible way of doing the stuff we did on Methods 1. It is also cooler. And more buzzwordy. 

TL;DR: GLM/GLMMs can do everything lm() and lmer() can do, just better and you'll iteratively learn why it's better across Methods 2-3-4, trust me :) 

## Piecemeal hints
### Ex. 1.2a) stepwise-instruction
The question says: Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 3.9. We're given a linear regression equation that specifies what y should be, and we're told what the x should be from and to in the question and the info from 1.2). Here's 8 smaller steps on how to solve it, step by step: 

#### 1) First, define the regression model parameters (intercept, slope, error) (hint: just make variables called 'intercept' and so on and define the number)

#### 3) Generate X values in the specified range using seq()

#### 4) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line

#### 5) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y value
s. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.

#### 6) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot

#### 7) Now, plot to check it out

#### 8) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data. 
